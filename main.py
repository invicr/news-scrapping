import json
import asyncio
from fastapi import FastAPI, File, UploadFile, Request, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from scraping import scrape_aitimes, scrape_venturebeat, scrape_techcrunch, scrape_zdnet
from fastapi.staticfiles import StaticFiles
from datetime import datetime
import os

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜ë¡œ URL ëª©ë¡ ì €ì¥
url_list = []
processed_urls = set()  # ì²˜ë¦¬ëœ URLì„ ì¶”ì í•˜ëŠ” ì „ì—­ ë³€ìˆ˜

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì • (ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•  ë•Œë§Œ ë§ˆìš´íŠ¸)
if os.path.exists("static"):
    app.mount("/static", StaticFiles(directory="static"), name="static")

# í…œí”Œë¦¿ ì„¤ì •
templates = Jinja2Templates(directory="templates")

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€ ë Œë”ë§"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/result", response_class=HTMLResponse)
async def result(request: Request, file: UploadFile = File(...)):
    """CSV íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ í˜ì´ì§€ ë Œë”ë§"""
    global url_list, processed_urls
    
    try:
        # CSV íŒŒì¼ ì½ê¸°
        content = await file.read()
        text = content.decode('utf-8-sig')  # UTF-8 with BOM ì²˜ë¦¬
        
        # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  ë¹ˆ ì¤„ ì œê±°
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        
        # URL ì¶”ì¶œ (ì²« ë²ˆì§¸ ì—´)
        urls = []
        seen_urls = set()  # ì¤‘ë³µ URL ì²´í¬ë¥¼ ìœ„í•œ set
        
        for line in lines:
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì²« ë²ˆì§¸ ì—´ë§Œ ê°€ì ¸ì˜¤ê¸°
            parts = line.split(',')
            if parts:  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°
                url = parts[0].strip().replace('\ufeff', '')  # BOM ì œê±°
                if url and url.startswith(('http://', 'https://')):  # URL í˜•ì‹ ê²€ì‚¬
                    # URL ì •ê·œí™” (httpì™€ https í†µì¼)
                    normalized_url = url.replace('http://', 'https://')
                    if normalized_url not in seen_urls:
                        seen_urls.add(normalized_url)
                        urls.append(url)
        
        if not urls:
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì— ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        valid_urls = [url for url in urls if any(domain in url for domain in 
                    ['aitimes.com', 'aitimes.kr', 'venturebeat.com', 'techcrunch.com', 'zdnet.co.kr'])]
        
        if not valid_urls:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ëŠ” ë„ë©”ì¸ì˜ URLì´ ì—†ìŠµë‹ˆë‹¤. (aitimes.com, venturebeat.com, techcrunch.com, zdnet.co.kr)")
        
        url_list = valid_urls
        processed_urls.clear()  # ìƒˆë¡œìš´ URL ëª©ë¡ì´ ë“¤ì–´ì˜¬ ë•Œ processed_urls ì´ˆê¸°í™”
        
        # ëª¨ë“  ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë˜í•‘
        articles = []
        batch_size = 5  # í•œ ë²ˆì— ì²˜ë¦¬í•  URL ìˆ˜
        
        print("\n=== URL ì²˜ë¦¬ ì‹œì‘ ===")
        print(f"ì´ URL ìˆ˜: {len(url_list)}ê°œ")
        print("=" * 50)
        
        # URLì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(url_list), batch_size):
            batch = url_list[i:i + batch_size]
            tasks = []
            batch_num = (i // batch_size) + 1
            
            print(f"\n[ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹œì‘]")
            print(f"ë°°ì¹˜ í¬ê¸°: {len(batch)}ê°œ")
            
            for url in batch:
                # URL ì •ê·œí™” ë° ì¤‘ë³µ ì²´í¬
                normalized_url = url.replace('http://', 'https://')
                if normalized_url in processed_urls:
                    print(f"â© ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°: {url}")
                    continue
                    
                processed_urls.add(normalized_url)
                print(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {url}")
                
                if "aitimes.com" in url or "aitimes.kr" in url:
                    tasks.append(asyncio.wait_for(scrape_aitimes(url), timeout=30))
                elif "venturebeat.com" in url:
                    tasks.append(asyncio.wait_for(scrape_venturebeat(url), timeout=30))
                elif "techcrunch.com" in url:
                    tasks.append(asyncio.wait_for(scrape_techcrunch(url), timeout=30))
                elif "zdnet.co.kr" in url:
                    tasks.append(asyncio.wait_for(scrape_zdnet(url), timeout=30))
            
            if not tasks:  # ëª¨ë“  URLì´ ì¤‘ë³µì¸ ê²½ìš°
                print("âš ï¸ ëª¨ë“  URLì´ ì¤‘ë³µë˜ì–´ ë°°ì¹˜ ê±´ë„ˆë›°ê¸°")
                continue
                
            try:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in batch_results:
                    if isinstance(result, Exception):
                        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(result)}")
                    else:
                        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {result.url}")
                        articles.append(result)
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
                
            print(f"[ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì™„ë£Œ]")
            print("-" * 50)
        
        if not articles:
            raise HTTPException(status_code=500, detail="ìŠ¤í¬ë˜í•‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print("\n=== URL ì²˜ë¦¬ ì™„ë£Œ ===")
        print(f"ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
        print("=" * 50)
        
        # í˜„ì¬ ë‚ ì§œ í¬ë§·íŒ…
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼ (%a)")
        current_date = current_date.replace("Mon", "ì›”").replace("Tue", "í™”").replace("Wed", "ìˆ˜").replace("Thu", "ëª©").replace("Fri", "ê¸ˆ").replace("Sat", "í† ").replace("Sun", "ì¼")
        # í˜„ì¬ ì—°ë„ì˜ ì£¼ì°¨ ê³„ì‚°
        current_week = datetime.now().isocalendar()[1]
        current_week = f"({current_week}ì£¼ì°¨)"
        
        return templates.TemplateResponse(
            "result_card.html",
            {
                "request": request,
                "articles": articles,
                "current_date": current_date,
                "current_week": current_week
            }
        )
            
    except UnicodeDecodeError:
        raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ UTF-8 í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
